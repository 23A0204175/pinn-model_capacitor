{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d66c9c-c155-48b8-afee-3cfd899bdee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2e880c-afb8-4307-8a42-6d931140920b",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ITERATIONS = 150\n",
    "EPOCHS_PER_ITERATION = 8000\n",
    "N_ADD_POINTS = 2000\n",
    "\n",
    "N_PDE_POINTS_PARAMETRIC = 16384\n",
    "N_BC_POINTS_PARAMETRIC = 8192\n",
    "\n",
    "BEST_PARAMS = {\n",
    "    'learning_rate': 0.000019,\n",
    "    'num_layers': 8,\n",
    "    'hidden_size': 384,\n",
    "    'W_DIRICHLET': 36.163964,\n",
    "    'W_NEUMANN': 1.926002,\n",
    "    'W_OVERSHOOT': 154.014293,\n",
    "    'W_DATA': 162.695670,\n",
    "    'W_E_FIELD_X': 6.170429,\n",
    "    'W_E_FIELD_Y': 17.506671,\n",
    "}\n",
    "\n",
    "LEARNING_RATE = BEST_PARAMS['learning_rate']\n",
    "W_DIRICHLET = BEST_PARAMS['W_DIRICHLET']\n",
    "W_NEUMANN = BEST_PARAMS['W_NEUMANN']\n",
    "W_OVERSHOOT = BEST_PARAMS['W_OVERSHOOT']\n",
    "W_DATA = BEST_PARAMS['W_DATA']\n",
    "W_E_FIELD_X = BEST_PARAMS['W_E_FIELD_X']\n",
    "W_E_FIELD_Y = BEST_PARAMS['W_E_FIELD_Y']\n",
    "NUM_LAYERS = BEST_PARAMS['num_layers']\n",
    "HIDDEN_SIZE = BEST_PARAMS['hidden_size']\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ae0c67-e6d2-434d-8d1d-168c4e847e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "L_range = [40.0, 110.0]; w_range = [2.0, 6.0]; d_range = [2.0, 8.0]\n",
    "lx, ly = 120.0, 22.5\n",
    "x_domain_phys = [-lx / 2, lx / 2]; y_domain_phys = [-ly / 2, ly / 2]\n",
    "V_top = 1.0; V_bottom = 0.0\n",
    "\n",
    "L_char_x = lx / 2.0\n",
    "L_char_y = ly / 2.0\n",
    "\n",
    "lx_n, ly_n = 2.0, 2.0\n",
    "x_domain_n = [-1.0, 1.0]; y_domain_n = [-1.0, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b3a405-8870-437f-8af4-2adca35495f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParametricPINN(nn.Module):\n",
    "    def __init__(self, num_layers=NUM_LAYERS, hidden_size=HIDDEN_SIZE):\n",
    "        super(ParametricPINN, self).__init__()\n",
    "        layers = [nn.Linear(5, hidden_size), nn.Tanh()]\n",
    "        for _ in range(num_layers - 1):\n",
    "            layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            layers.append(nn.Tanh())\n",
    "        layers.append(nn.Linear(hidden_size, 1))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x_n, y_n, L_n, w_n, d_n):\n",
    "        batch_size = x_n.shape[0]\n",
    "        if L_n.numel() == 1: L_n = L_n.expand(batch_size, 1)\n",
    "        if w_n.numel() == 1: w_n = w_n.expand(batch_size, 1)\n",
    "        if d_n.numel() == 1: d_n = d_n.expand(batch_size, 1)\n",
    "        input_features = torch.cat([\n",
    "            x_n.view(-1, 1), y_n.view(-1, 1),\n",
    "            L_n.view(-1, 1), w_n.view(-1, 1), d_n.view(-1, 1)\n",
    "        ], dim=1)\n",
    "        return self.net(input_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a59ae7-b831-45fc-8dcb-f2764f58b598",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_inside_plates_parametric(x_n, y_n, L_n, w_n, d_n, L_char_x, L_char_y):\n",
    "    scale_ratio = L_char_x / L_char_y\n",
    "    w_n_y_scaled = w_n * scale_ratio\n",
    "    d_n_y_scaled = d_n * scale_ratio\n",
    "    \n",
    "    plate_x_min_n = -L_n / 2; plate_x_max_n = L_n / 2\n",
    "    top_plate_y_min_n = d_n_y_scaled / 2; top_plate_y_max_n = d_n_y_scaled / 2 + w_n_y_scaled\n",
    "    bottom_plate_y_min_n = -d_n_y_scaled / 2 - w_n_y_scaled; bottom_plate_y_max_n = -d_n_y_scaled / 2\n",
    "    \n",
    "    in_top = (x_n >= plate_x_min_n) & (x_n <= plate_x_max_n) & (y_n >= top_plate_y_min_n) & (y_n <= top_plate_y_max_n)\n",
    "    in_bottom = (x_n >= plate_x_min_n) & (x_n <= plate_x_max_n) & (y_n >= bottom_plate_y_min_n) & (y_n <= bottom_plate_y_max_n)\n",
    "    return in_bottom | in_top\n",
    "\n",
    "def sample_params(batch_size, device, L_char_x):\n",
    "    L_phys = torch.rand(batch_size, 1, device=device) * (L_range[1] - L_range[0]) + L_range[0]\n",
    "    w_phys = torch.rand(batch_size, 1, device=device) * (w_range[1] - w_range[0]) + w_range[0]\n",
    "    d_phys = torch.rand(batch_size, 1, device=device) * (d_range[1] - d_range[0]) + d_range[0]\n",
    "    return L_phys / L_char_x, w_phys / L_char_x, d_phys / L_char_x\n",
    "\n",
    "def generate_pde_points_parametric(n_points, device, L_char_x, L_char_y):\n",
    "    x_pde_n_list, y_pde_n_list, L_n_list, w_n_list, d_n_list = [], [], [], [], []\n",
    "    num_generated = 0\n",
    "    while num_generated < n_points:\n",
    "        num_candidates = int(n_points * 1.5) + 1\n",
    "        L_cand_n, w_cand_n, d_cand_n = sample_params(num_candidates, device, L_char_x)\n",
    "        x_cand_n = torch.rand(num_candidates, 1, device=device) * 2 - 1\n",
    "        y_cand_n = torch.rand(num_candidates, 1, device=device) * 2 - 1\n",
    "        is_outside = ~is_inside_plates_parametric(x_cand_n, y_cand_n, L_cand_n, w_cand_n, d_cand_n, L_char_x, L_char_y)\n",
    "        x_pde_n_list.append(x_cand_n[is_outside]); y_pde_n_list.append(y_cand_n[is_outside])\n",
    "        L_n_list.append(L_cand_n[is_outside]); w_n_list.append(w_cand_n[is_outside]); d_n_list.append(d_cand_n[is_outside])\n",
    "        num_generated += is_outside.sum().item()\n",
    "    x_pde = torch.cat(x_pde_n_list, dim=0)[:n_points]; y_pde = torch.cat(y_pde_n_list, dim=0)[:n_points]\n",
    "    L_pde = torch.cat(L_n_list, dim=0)[:n_points]; w_pde = torch.cat(w_n_list, dim=0)[:n_points]; d_pde = torch.cat(d_n_list, dim=0)[:n_points]\n",
    "    return (x_pde, y_pde), (L_pde, w_pde, d_pde)\n",
    "\n",
    "def generate_bc_points_parametric(N_bc, L_n, w_n, d_n, device, L_char_x, L_char_y):\n",
    "    scale_ratio = L_char_x / L_char_y\n",
    "    w_n_y_scaled = w_n * scale_ratio\n",
    "    d_n_y_scaled = d_n * scale_ratio\n",
    "    n_plates_total = N_bc // 2; n_neumann_total = N_bc - n_plates_total\n",
    "    n_per_plate = n_plates_total // 2; n_plate_side = max(1, n_per_plate // 4)\n",
    "    n_outer_side = max(1, n_neumann_total // 4)\n",
    "    plate_x_range_n_dyn = [-L_n / 2, L_n / 2]\n",
    "    top_plate_y_range_n_dyn = [d_n_y_scaled / 2, d_n_y_scaled / 2 + w_n_y_scaled]\n",
    "    bottom_plate_y_range_n_dyn = [-d_n_y_scaled / 2 - w_n_y_scaled, -d_n_y_scaled / 2]\n",
    "    x_t_t = torch.rand(n_plate_side,1,device=device)*L_n+plate_x_range_n_dyn[0]; y_t_t=torch.ones_like(x_t_t)*top_plate_y_range_n_dyn[1]\n",
    "    x_t_b = torch.rand(n_plate_side,1,device=device)*L_n+plate_x_range_n_dyn[0]; y_t_b=torch.ones_like(x_t_b)*top_plate_y_range_n_dyn[0]\n",
    "    y_t_l=torch.rand(n_plate_side,1,device=device)*w_n_y_scaled+top_plate_y_range_n_dyn[0]; x_t_l=torch.ones_like(y_t_l)*plate_x_range_n_dyn[0]\n",
    "    y_t_r=torch.rand(n_plate_side,1,device=device)*w_n_y_scaled+top_plate_y_range_n_dyn[0]; x_t_r=torch.ones_like(y_t_r)*plate_x_range_n_dyn[1]\n",
    "    x_top=torch.cat([x_t_t,x_t_b,x_t_l,x_t_r]); y_top=torch.cat([y_t_t,y_t_b,y_t_l,y_t_r])\n",
    "    x_b_t=torch.rand(n_plate_side,1,device=device)*L_n+plate_x_range_n_dyn[0]; y_b_t=torch.ones_like(x_b_t)*bottom_plate_y_range_n_dyn[1]\n",
    "    x_b_b=torch.rand(n_plate_side,1,device=device)*L_n+plate_x_range_n_dyn[0]; y_b_b=torch.ones_like(x_b_b)*bottom_plate_y_range_n_dyn[0]\n",
    "    y_b_l=torch.rand(n_plate_side,1,device=device)*w_n_y_scaled+bottom_plate_y_range_n_dyn[0]; x_b_l=torch.ones_like(y_b_l)*plate_x_range_n_dyn[0]\n",
    "    y_b_r=torch.rand(n_plate_side,1,device=device)*w_n_y_scaled+bottom_plate_y_range_n_dyn[0]; x_b_r=torch.ones_like(y_b_r)*plate_x_range_n_dyn[1]\n",
    "    x_bottom=torch.cat([x_b_t,x_b_b,x_b_l,x_b_r]); y_bottom=torch.cat([y_b_t,y_b_b,y_b_l,y_b_r])\n",
    "    x_n_l=torch.full((n_outer_side,1),-1.0,device=device); y_n_l=torch.rand(n_outer_side,1,device=device)*2-1\n",
    "    x_n_r=torch.full((n_outer_side,1),1.0,device=device); y_n_r=torch.rand(n_outer_side,1,device=device)*2-1\n",
    "    y_n_b=torch.full((n_outer_side,1),-1.0,device=device); x_n_b=torch.rand(n_outer_side,1,device=device)*2-1\n",
    "    y_n_t=torch.full((n_outer_side,1),1.0,device=device); x_n_t=torch.rand(n_outer_side,1,device=device)*2-1\n",
    "    x_neumann=torch.cat([x_n_l,x_n_r,x_n_b,x_n_t]); y_neumann=torch.cat([y_n_l,y_n_r,y_n_b,y_n_t])\n",
    "    return {'top_plate': (x_top, y_top), 'bottom_plate': (x_bottom, y_bottom), 'neumann': (x_neumann, y_neumann)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b97b304-eac4-48f3-9dc9-448e6997c4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_parametric(model, pde_points_n, bc_points_n, corrective_data, params_dict, E_norm_factor):\n",
    "    x_pde_n, y_pde_n = pde_points_n\n",
    "    L_pde_n, w_pde_n, d_pde_n = params_dict['pde']\n",
    "    x_pde_n.requires_grad_(True); y_pde_n.requires_grad_(True)\n",
    "    V_pde = model(x_pde_n, y_pde_n, L_pde_n, w_pde_n, d_pde_n)\n",
    "    first_grads_pde = torch.autograd.grad(V_pde.sum(), [x_pde_n, y_pde_n], create_graph=True)\n",
    "    V_x_n, V_y_n = first_grads_pde[0], first_grads_pde[1]\n",
    "    V_xx_n = torch.autograd.grad(V_x_n.sum(), x_pde_n, retain_graph=True)[0]\n",
    "    V_yy_n = torch.autograd.grad(V_y_n.sum(), y_pde_n)[0]\n",
    "    loss_pde = torch.mean((V_xx_n + V_yy_n)**2)\n",
    "    L_bc_n, w_bc_n, d_bc_n = params_dict['bc']\n",
    "    x_top_n, y_top_n = bc_points_n['top_plate']\n",
    "    loss_bc_top = torch.mean((model(x_top_n, y_top_n, L_bc_n, w_bc_n, d_bc_n) - V_top)**2)\n",
    "    x_bottom_n, y_bottom_n = bc_points_n['bottom_plate']\n",
    "    loss_bc_bottom = torch.mean((model(x_bottom_n, y_bottom_n, L_bc_n, w_bc_n, d_bc_n) - V_bottom)**2)\n",
    "    loss_bc_dirichlet = loss_bc_top + loss_bc_bottom\n",
    "    x_n, y_n = bc_points_n['neumann']\n",
    "    x_n.requires_grad_(True); y_n.requires_grad_(True)\n",
    "    V_n = model(x_n, y_n, L_bc_n, w_bc_n, d_bc_n)\n",
    "    first_grads_neu = torch.autograd.grad(V_n.sum(), [x_n, y_n], create_graph=False)\n",
    "    V_nx_n, V_ny_n = first_grads_neu[0], first_grads_neu[1]\n",
    "    is_lr = (torch.abs(x_n - x_domain_n[0]) < 1e-6) | (torch.abs(x_n - x_domain_n[1]) < 1e-6)\n",
    "    is_tb = (torch.abs(y_n - y_domain_n[0]) < 1e-6) | (torch.abs(y_n - y_domain_n[1]) < 1e-6)\n",
    "    loss_neumann = torch.mean(V_nx_n[is_lr]**2) + torch.mean(V_ny_n[is_tb]**2)\n",
    "    x_corr_n, y_corr_n, v_corr, ex_corr_n_scaled, ey_corr_n_scaled, L_corr_n, w_corr_n, d_corr_n = corrective_data\n",
    "    if x_corr_n.shape[0] > 0:\n",
    "        x_corr_n.requires_grad_(True); y_corr_n.requires_grad_(True)\n",
    "        v_pred_corr = model(x_corr_n, y_corr_n, L_corr_n, w_corr_n, d_corr_n)\n",
    "        loss_data = torch.mean((v_pred_corr - v_corr)**2)\n",
    "        grads_v_corr = torch.autograd.grad(v_pred_corr.sum(), [x_corr_n, y_corr_n], create_graph=True)\n",
    "        dvdx_pred_n, dvdy_pred_n = grads_v_corr[0], grads_v_corr[1]\n",
    "        ex_pred_n = -dvdx_pred_n; ey_pred_n = -dvdy_pred_n\n",
    "        ex_pred_n_scaled = ex_pred_n / E_norm_factor\n",
    "        ey_pred_n_scaled = ey_pred_n / E_norm_factor\n",
    "        mse_ex = torch.mean((ex_pred_n_scaled - ex_corr_n_scaled)**2)\n",
    "        mse_ey = torch.mean((ey_pred_n_scaled - ey_corr_n_scaled)**2)\n",
    "        loss_E = W_E_FIELD_X * mse_ex + W_E_FIELD_Y * mse_ey\n",
    "    else:\n",
    "        loss_data = torch.tensor(0.0, device=device); loss_E = torch.tensor(0.0, device=device)\n",
    "    v_pred_all_pde = model(x_pde_n, y_pde_n, L_pde_n, w_pde_n, d_pde_n)\n",
    "    loss_overshoot = torch.mean(torch.relu(v_pred_all_pde - 1.0) + torch.relu(-v_pred_all_pde))\n",
    "    loss_total = loss_pde + W_DIRICHLET * loss_bc_dirichlet + W_NEUMANN * loss_neumann + \\\n",
    "                 W_OVERSHOOT * loss_overshoot + W_DATA * loss_data + loss_E\n",
    "    return loss_total, loss_pde, loss_bc_dirichlet, loss_neumann, loss_overshoot, loss_data, loss_E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e5bf73-3536-47f4-a346-20caafb4bee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_param_sets = [\n",
    "    {'L': 40, 'w': 2, 'd': 2}, {'L': 40, 'w': 3.3, 'd': 2}, {'L': 40, 'w': 4.7, 'd': 2}, {'L': 40, 'w': 6, 'd': 2},\n",
    "    {'L': 63.3, 'w': 2, 'd': 2}, {'L': 63.3, 'w': 3.3, 'd': 2}, {'L': 63.3, 'w': 4.7, 'd': 2}, {'L': 63.3, 'w': 6, 'd': 2},\n",
    "    {'L': 86.7, 'w': 2, 'd': 2}, {'L': 86.7, 'w': 3.3, 'd': 2}, {'L': 86.7, 'w': 4.7, 'd': 2}, {'L': 86.7, 'w': 6, 'd': 2},\n",
    "    {'L': 110, 'w': 2, 'd': 2}, {'L': 110, 'w': 3.3, 'd': 2}, {'L': 110, 'w': 4.7, 'd': 2}, {'L': 110, 'w': 6, 'd': 2},\n",
    "    {'L': 40, 'w': 2, 'd': 4}, {'L': 40, 'w': 3.3, 'd': 4}, {'L': 40, 'w': 4.7, 'd': 4}, {'L': 40, 'w': 6, 'd': 4},\n",
    "    {'L': 63.3, 'w': 2, 'd': 4}, {'L': 63.3, 'w': 3.3, 'd': 4}, {'L': 63.3, 'w': 4.7, 'd': 4}, {'L': 63.3, 'w': 6, 'd': 4},\n",
    "    {'L': 86.7, 'w': 2, 'd': 4}, {'L': 86.7, 'w': 3.3, 'd': 4}, {'L': 86.7, 'w': 4.7, 'd': 4}, {'L': 86.7, 'w': 6, 'd': 4},\n",
    "    {'L': 110, 'w': 2, 'd': 4}, {'L': 110, 'w': 3.3, 'd': 4}, {'L': 110, 'w': 4.7, 'd': 4}, {'L': 110, 'w': 6, 'd': 4},\n",
    "    {'L': 40, 'w': 2, 'd': 6}, {'L': 40, 'w': 3.3, 'd': 6}, {'L': 40, 'w': 4.7, 'd': 6}, {'L': 40, 'w': 6, 'd': 6},\n",
    "    {'L': 63.3, 'w': 2, 'd': 6}, {'L': 63.3, 'w': 3.3, 'd': 6}, {'L': 63.3, 'w': 4.7, 'd': 6}, {'L': 63.3, 'w': 6, 'd': 6},\n",
    "    {'L': 86.7, 'w': 2, 'd': 6}, {'L': 86.7, 'w': 3.3, 'd': 6}, {'L': 86.7, 'w': 4.7, 'd': 6}, {'L': 86.7, 'w': 6, 'd': 6},\n",
    "    {'L': 110, 'w': 2, 'd': 6}, {'L': 110, 'w': 3.3, 'd': 6}, {'L': 110, 'w': 4.7, 'd': 6}, {'L': 110, 'w': 6, 'd': 6},\n",
    "    {'L': 40, 'w': 2, 'd': 8}, {'L': 40, 'w': 3.3, 'd': 8}, {'L': 40, 'w': 4.7, 'd': 8}, {'L': 40, 'w': 6, 'd': 8},\n",
    "    {'L': 63.3, 'w': 2, 'd': 8}, {'L': 63.3, 'w': 3.3, 'd': 8}, {'L': 63.3, 'w': 4.7, 'd': 8}, {'L': 63.3, 'w': 6, 'd': 8},\n",
    "    {'L': 86.7, 'w': 2, 'd': 8}, {'L': 86.7, 'w': 3.3, 'd': 8}, {'L': 86.7, 'w': 4.7, 'd': 8}, {'L': 86.7, 'w': 6, 'd': 8},\n",
    "    {'L': 110, 'w': 2, 'd': 8}, {'L': 110, 'w': 3.3, 'd': 8}, {'L': 110, 'w': 4.7, 'd': 8}, {'L': 110, 'w': 6, 'd': 8},\n",
    "]\n",
    "\n",
    "all_dataframes = []\n",
    "for params in train_param_sets:\n",
    "    L_val, w_val, d_val = params['L'], params['w'], params['d']\n",
    "    d_val_str = str(d_val).replace('.', '_')\n",
    "    filename = f\"comsol_V_Ex_Ey_C_L{L_val}_w{w_val}_d{d_val_str}.csv\"\n",
    "    try:\n",
    "        df = pd.read_csv(filename); df['L'] = L_val; df['w'] = w_val; df['d'] = d_val\n",
    "        all_dataframes.append(df)\n",
    "        print(f\"Loaded successfully: {filename}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Not found: {filename}\")\n",
    "\n",
    "if not all_dataframes:\n",
    "    print(\"Failed to load any data files\"); exit()\n",
    "master_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "df_cleaned = master_df.dropna()\n",
    "\n",
    "x_db_phys = torch.tensor(df_cleaned['X'].values,dtype=torch.float32).view(-1,1).to(device)\n",
    "y_db_phys = torch.tensor(df_cleaned['Y'].values,dtype=torch.float32).view(-1,1).to(device)\n",
    "v_db = torch.tensor(df_cleaned['Potential'].values,dtype=torch.float32).view(-1,1).to(device)\n",
    "L_db_phys = torch.tensor(df_cleaned['L'].values,dtype=torch.float32).view(-1,1).to(device)\n",
    "w_db_phys = torch.tensor(df_cleaned['w'].values,dtype=torch.float32).view(-1,1).to(device)\n",
    "d_db_phys = torch.tensor(df_cleaned['d'].values,dtype=torch.float32).view(-1,1).to(device)\n",
    "ex_db_phys = torch.tensor(df_cleaned['Ex'].values,dtype=torch.float32).view(-1,1).to(device)\n",
    "ey_db_phys = torch.tensor(df_cleaned['Ey'].values,dtype=torch.float32).view(-1,1).to(device)\n",
    "\n",
    "x_db_n = x_db_phys / L_char_x; y_db_n = y_db_phys / L_char_y\n",
    "L_db_n = L_db_phys / L_char_x; w_db_n = w_db_phys / L_char_x; d_db_n = d_db_phys / L_char_x\n",
    "ex_db_n = ex_db_phys * L_char_x; ey_db_n = ey_db_phys * L_char_y\n",
    "\n",
    "E_norm_factor = max(torch.abs(ex_db_n).max(), torch.abs(ey_db_n).max())\n",
    "ex_db_n_scaled = ex_db_n / E_norm_factor\n",
    "ey_db_n_scaled = ey_db_n / E_norm_factor\n",
    "print(f\"E_norm_factor: {E_norm_factor.item()}\")\n",
    "print(f\"Total number of database points: {len(df_cleaned)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d801b1f-023b-47e0-8788-ad3ae38b942b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinn_model = ParametricPINN(num_layers=NUM_LAYERS, hidden_size=HIDDEN_SIZE).to(device)\n",
    "optimizer = torch.optim.Adam(pinn_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "start_iteration = 0\n",
    "corrective_set_n = torch.empty((0, 8), device=device)\n",
    "checkpoint_path = 'training_checkpoint.pth'\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(f\"Found breakpoint file: {checkpoint_path}\")\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    pinn_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_iteration = checkpoint['iteration']\n",
    "    corrective_set_n = checkpoint['corrective_set_n'].to(device)\n",
    "    print(f\"Continue training from {start_iteration + 1} iteration\")\n",
    "\n",
    "total_start_time = time.time()\n",
    "for iteration in range(start_iteration, N_ITERATIONS):\n",
    "    print(f\"\\n{'='*20} Number of iterations {iteration+1}/{N_ITERATIONS} {'='*20}\")\n",
    "    \n",
    "    pinn_model.train()\n",
    "    iter_start_time = time.time()\n",
    "    for epoch in range(EPOCHS_PER_ITERATION):\n",
    "        pde_points_n, pde_params_n = generate_pde_points_parametric(N_PDE_POINTS_PARAMETRIC, device, L_char_x, L_char_y)\n",
    "        L_bc_n, w_bc_n, d_bc_n = sample_params(1, device, L_char_x)\n",
    "        bc_points_n = generate_bc_points_parametric(N_BC_POINTS_PARAMETRIC, L_bc_n, w_bc_n, d_bc_n, device, L_char_x, L_char_y)\n",
    "        params_dict = {'pde': pde_params_n, 'bc': (L_bc_n, w_bc_n, d_bc_n)}\n",
    "\n",
    "        if corrective_set_n.shape[0] > 0:\n",
    "            sample_size = min(N_ADD_POINTS, corrective_set_n.shape[0])\n",
    "            sample_indices = torch.randint(0, corrective_set_n.shape[0], (sample_size,))\n",
    "            corrective_batch = corrective_set_n[sample_indices]\n",
    "            x_corr_n, y_corr_n, v_corr, ex_corr_n_scaled, ey_corr_n_scaled, L_corr_n, w_corr_n, d_corr_n = torch.split(corrective_batch, 1, dim=1)\n",
    "        else:\n",
    "            x_corr_n, y_corr_n, v_corr, ex_corr_n_scaled, ey_corr_n_scaled, L_corr_n, w_corr_n, d_corr_n = [torch.empty((0,1), device=device)] * 8\n",
    "        corrective_data = (x_corr_n, y_corr_n, v_corr, ex_corr_n_scaled, ey_corr_n_scaled, L_corr_n, w_corr_n, d_corr_n)\n",
    "        \n",
    "        loss, pde, diri, neu, over, data, loss_e = compute_loss_parametric(pinn_model, pde_points_n, bc_points_n, corrective_data, params_dict, E_norm_factor)\n",
    "        \n",
    "        if torch.isnan(loss):\n",
    "            print(f\"Loss became NAN in the epoch {epoch},stop training\")\n",
    "            break\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(pinn_model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 1000 == 0:\n",
    "            print(f\"  Epoch {epoch:5d} | L:{loss.item():.2e} | PDE:{pde.item():.2e} | Diri:{diri.item():.2e} | Neu:{neu.item():.2e} | Over:{over.item():.2e} | Data:{data.item():.2e} | Loss_E:{loss_e.item():.2e}\")\n",
    "    \n",
    "    if 'loss' in locals() and torch.isnan(loss):\n",
    "        break\n",
    "        \n",
    "    print(f\" This round of training took time: {time.time() - iter_start_time:.2f} seconds\")\n",
    "\n",
    "    pinn_model.eval()\n",
    "    \n",
    "    v_pred_db_list = []\n",
    "    batch_size = 16384\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, x_db_n.shape[0], batch_size):\n",
    "            v_pred_batch = pinn_model(x_db_n[i:i+batch_size], y_db_n[i:i+batch_size],\n",
    "                                      L_db_n[i:i+batch_size], w_db_n[i:i+batch_size], d_db_n[i:i+batch_size])\n",
    "            v_pred_db_list.append(v_pred_batch)\n",
    "    v_pred_db = torch.cat(v_pred_db_list, dim=0)\n",
    "    \n",
    "    errors = torch.abs(v_pred_db - v_db)\n",
    "    max_error = torch.max(errors)\n",
    "    print(f\" Current Maximum Error: {max_error.item():.4f} V\")\n",
    "    \n",
    "    _, indices = torch.topk(errors.flatten(), N_ADD_POINTS)\n",
    "    new_points_to_add = torch.cat([\n",
    "        x_db_n[indices], y_db_n[indices], v_db[indices],\n",
    "        ex_db_n_scaled[indices], ey_db_n_scaled[indices],\n",
    "        L_db_n[indices], w_db_n[indices], d_db_n[indices]\n",
    "    ], dim=1)\n",
    "    \n",
    "    corrective_set_n = torch.cat([corrective_set_n, new_points_to_add])\n",
    "    \n",
    "    unique_points = torch.unique(corrective_set_n.cpu(), dim=0).to(device)\n",
    "    corrective_set_n = unique_points\n",
    "    \n",
    "    print(f\" New point has been added to the correction set.The current size of the correction set is: {corrective_set_n.shape[0]}\")\n",
    "    \n",
    "    print(\"Save the current training progress to the checkpoint file\")\n",
    "    torch.save({\n",
    "        'iteration': iteration + 1,\n",
    "        'model_state_dict': pinn_model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'corrective_set_n': corrective_set_n.cpu(),\n",
    "    }, checkpoint_path)\n",
    "    print(\"Checkpoint saved successfully\")\n",
    "\n",
    "print(f\"\\ntotal training time spent: {time.time() - total_start_time:.2f} seconds\")\n",
    "torch.save(pinn_model.state_dict(), 'final_parametric_model.pth')\n",
    "print(\"The model has been saved to'final_parametric_model.pth'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be6652e-9beb-4182-9964-44a17874ced1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preload_comsol_data(param_sets_to_load, device):\n",
    "    ground_truth_data = {}\n",
    "    print(\"Preloading comsol data\")\n",
    "    for params in param_sets_to_load:\n",
    "        L, w, d = params['L'], params['w'], params['d']\n",
    "        #d_str = str(d).replace('.', '_')\n",
    "        filename = f\"comsol_V_Ex_Ey_C_L{L}_w{w}_d{d}.csv\"\n",
    "        try:\n",
    "            df = pd.read_csv(filename)\n",
    "            if len(df) != 80601:\n",
    "                print(f\"the file {filename} line count is not 80601\")\n",
    "                continue\n",
    "            nx, ny = 401, 201\n",
    "            v_matrix = df['Potential'].values.reshape((ny, nx), order='F')\n",
    "            ex_matrix = df['Ex'].values.reshape((ny, nx), order='F')\n",
    "            ey_matrix = df['Ey'].values.reshape((ny, nx), order='F')\n",
    "            v_comsol = torch.tensor(v_matrix, dtype=torch.float32).to(device)\n",
    "            ex_comsol = torch.tensor(ex_matrix, dtype=torch.float32).to(device)\n",
    "            ey_comsol = torch.tensor(ey_matrix, dtype=torch.float32).to(device)\n",
    "            c_comsol = df.dropna()['Capacitance_fF'].iloc[0]\n",
    "            ground_truth_data[(L, w, d)] = {\n",
    "                'V': v_comsol, 'Ex': ex_comsol, 'Ey': ey_comsol, 'C': c_comsol\n",
    "            }\n",
    "            print(f\"Preload successfully: {filename}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Not found: {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error while processing the file{filename} : {e}\")\n",
    "    return ground_truth_data\n",
    "\n",
    "def customize_colorbar_ticks(cbar, data_array):\n",
    "    clean_data = data_array[~np.isnan(data_array)]\n",
    "    if clean_data.size == 0: return\n",
    "    min_val, max_val = np.min(clean_data), np.max(clean_data)\n",
    "    default_ticks = cbar.get_ticks()\n",
    "    new_ticks = {min_val, max_val}\n",
    "    for tick in default_ticks:\n",
    "        if min_val < tick < max_val: new_ticks.add(tick)\n",
    "    sorted_ticks = sorted(list(new_ticks))\n",
    "    cbar.set_ticks(sorted_ticks)\n",
    "    cbar.ax.set_yticklabels([f'{t:.2e}' if abs(t) > 1000 else f'{t:.3f}' for t in sorted_ticks])\n",
    "\n",
    "def is_inside_gap_parametric(x_n, y_n, L_n, d_n, L_char_x, L_char_y):\n",
    "    scale_ratio = L_char_x / L_char_y\n",
    "    d_n_y_scaled = d_n * scale_ratio\n",
    "    gap_x_min_n = -L_n / 2\n",
    "    gap_x_max_n = L_n / 2\n",
    "    gap_y_min_n = -d_n_y_scaled / 2\n",
    "    gap_y_max_n = d_n_y_scaled / 2\n",
    "    return (x_n >= gap_x_min_n) & (x_n <= gap_x_max_n) & \\\n",
    "           (y_n >= gap_y_min_n) & (y_n <= gap_y_max_n)\n",
    "\n",
    "def generate_points_for_fringe(n_points, L, w, d, device, L_char_x, L_char_y):\n",
    "    L_n_val = L / L_char_x\n",
    "    w_n_val = w / L_char_x\n",
    "    d_n_val = d / L_char_x\n",
    "    x_fringe_n_list, y_fringe_n_list = [], []\n",
    "    num_generated = 0\n",
    "    while num_generated < n_points:\n",
    "        needed = n_points - num_generated\n",
    "        num_candidates = int(needed * 2.0) + 100\n",
    "        x_cand_n = torch.rand(num_candidates, 1, device=device) * 2 - 1\n",
    "        y_cand_n = torch.rand(num_candidates, 1, device=device) * 2 - 1\n",
    "        L_cand_n = torch.full_like(x_cand_n, L_n_val)\n",
    "        w_cand_n = torch.full_like(x_cand_n, w_n_val)\n",
    "        d_cand_n = torch.full_like(x_cand_n, d_n_val)\n",
    "        is_outside_plates = ~is_inside_plates_parametric(x_cand_n, y_cand_n, L_cand_n, w_cand_n, d_cand_n, L_char_x, L_char_y)\n",
    "        is_outside_gap = ~is_inside_gap_parametric(x_cand_n, y_cand_n, L_cand_n, d_cand_n, L_char_x, L_char_y)\n",
    "        valid_mask = is_outside_plates & is_outside_gap\n",
    "        x_fringe_n_list.append(x_cand_n[valid_mask])\n",
    "        y_fringe_n_list.append(y_cand_n[valid_mask])\n",
    "        num_generated += valid_mask.sum().item()\n",
    "    x_fringe_n = torch.cat(x_fringe_n_list)[:n_points]\n",
    "    y_fringe_n = torch.cat(y_fringe_n_list)[:n_points]\n",
    "    return x_fringe_n, y_fringe_n\n",
    "\n",
    "def calculate_capacitance_parametric_final(model, L, w, d, L_char_x, L_char_y):\n",
    "    print(\" Calculating capacitance\")\n",
    "    model.eval()\n",
    "    epsilon_0 = 8.854187817e-12; epsilon = epsilon_0 * 1.0\n",
    "    um_to_m = 1e-6; delta_V = 1.0\n",
    "    Z_depth = 60; Z_depth_m = Z_depth * um_to_m\n",
    "    L_char_x_m = L_char_x * um_to_m\n",
    "    L_char_y_m = L_char_y * um_to_m\n",
    "    L_n_test = torch.tensor([[L / L_char_x]], device=device)\n",
    "    w_n_test = torch.tensor([[w / L_char_x]], device=device)\n",
    "    d_n_test = torch.tensor([[d / L_char_x]], device=device)\n",
    "    n_energy_samples = 2000000\n",
    "    n_gap_samples = int(n_energy_samples * 0.8)\n",
    "    n_fringe_samples = n_energy_samples - n_gap_samples\n",
    "    gap_area_m2 = (L * um_to_m) * (d * um_to_m)\n",
    "    total_air_area_m2 = (lx * um_to_m * ly * um_to_m) - (L * um_to_m * w * um_to_m * 2)\n",
    "    fringe_area_m2 = total_air_area_m2 - gap_area_m2\n",
    "    if fringe_area_m2 <= 0: fringe_area_m2 = 1e-12\n",
    "    L_n_val, d_n_val = L / L_char_x, d / L_char_x\n",
    "    gap_x_min_n, gap_x_max_n = -L_n_val / 2, L_n_val / 2\n",
    "    gap_y_min_n_mapped = (-d / 2) / L_char_y\n",
    "    gap_y_max_n_mapped = (d / 2) / L_char_y\n",
    "    x_gap_n = (torch.rand(n_gap_samples, 1, device=device) * (gap_x_max_n - gap_x_min_n) + gap_x_min_n).requires_grad_(True)\n",
    "    y_gap_n = (torch.rand(n_gap_samples, 1, device=device) * (gap_y_max_n_mapped - gap_y_min_n_mapped) + gap_y_min_n_mapped).requires_grad_(True)\n",
    "    x_fringe_n, y_fringe_n = generate_points_for_fringe(n_fringe_samples, L, w, d, device, L_char_x, L_char_y)\n",
    "    x_fringe_n = x_fringe_n.view(-1, 1).requires_grad_(True)\n",
    "    y_fringe_n = y_fringe_n.view(-1, 1).requires_grad_(True)\n",
    "\n",
    "    def get_energy_sum(x_points, y_points):\n",
    "        energy_sum = 0.0\n",
    "        batch_size = 16384\n",
    "        for i in range(0, x_points.shape[0], batch_size):\n",
    "            x_batch_n, y_batch_n = x_points[i:i+batch_size], y_points[i:i+batch_size]\n",
    "            V_batch = model(x_batch_n, y_batch_n, L_n_test, w_n_test, d_n_test)\n",
    "            grads_n = torch.autograd.grad(V_batch.sum(), [x_batch_n, y_batch_n], create_graph=False)\n",
    "            dVdx_n, dVdy_n = grads_n[0], grads_n[1]\n",
    "            grad_V_sq_phys = (dVdx_n**2 / L_char_x_m**2) + (dVdy_n**2 / L_char_y_m**2)\n",
    "            u_batch = 0.5 * epsilon * grad_V_sq_phys\n",
    "            energy_sum += torch.sum(u_batch).item()\n",
    "        return energy_sum\n",
    "    \n",
    "    U_per_depth_gap = (get_energy_sum(x_gap_n, y_gap_n) / n_gap_samples if n_gap_samples > 0 else 0) * gap_area_m2\n",
    "    U_per_depth_fringe = (get_energy_sum(x_fringe_n, y_fringe_n) / n_fringe_samples if n_fringe_samples > 0 else 0) * fringe_area_m2\n",
    "    \n",
    "    U_per_depth = U_per_depth_gap + U_per_depth_fringe\n",
    "    Capacitance_from_U = (2 * U_per_depth * Z_depth_m) / (delta_V**2)\n",
    "    return 0.0, Capacitance_from_U * 1e15\n",
    "\n",
    "def analyze_gradient_model_case(model, L, w, d, ground_truth, L_char_x, L_char_y):\n",
    "    print(f\"\\n--- analyse L={L}, w={w}, d={d} \")\n",
    "    model.eval()\n",
    "\n",
    "    gt = ground_truth.get((L, w, d))\n",
    "    if gt is None:\n",
    "        print(\"No comsol data in preload\")\n",
    "        return\n",
    "    V_comsol, Ex_comsol, Ey_comsol, C_comsol_fF = gt['V'], gt['Ex'], gt['Ey'], gt['C']\n",
    "\n",
    "    nx, ny = 401, 201\n",
    "    x_plot_phys = torch.linspace(x_domain_phys[0], x_domain_phys[1], nx, device=device)\n",
    "    y_plot_phys = torch.linspace(y_domain_phys[0], y_domain_phys[1], ny, device=device)\n",
    "    X_phys, Y_phys = torch.meshgrid(x_plot_phys, y_plot_phys, indexing='xy')\n",
    "    \n",
    "    X_n_plot = X_phys / L_char_x\n",
    "    Y_n_plot = Y_phys / L_char_y\n",
    "    X_n_plot.requires_grad_(True); Y_n_plot.requires_grad_(True)\n",
    "    \n",
    "    L_n_test = torch.tensor([[L / L_char_x]], device=device)\n",
    "    w_n_test = torch.tensor([[w / L_char_x]], device=device)\n",
    "    d_n_test = torch.tensor([[d / L_char_x]], device=device)\n",
    "\n",
    "    V_pred_flat = model(X_n_plot.flatten().view(-1,1), Y_n_plot.flatten().view(-1,1), L_n_test, w_n_test, d_n_test)\n",
    "    V_pred = V_pred_flat.reshape(ny, nx)\n",
    "    \n",
    "    grad_V_n = torch.autograd.grad(V_pred.sum(), [X_n_plot, Y_n_plot], create_graph=False)\n",
    "    dVdx_n, dVdy_n = grad_V_n[0], grad_V_n[1]\n",
    "    \n",
    "    um_to_m = 1e-6\n",
    "    Ex_pred_phys = -dVdx_n / (L_char_x * um_to_m)\n",
    "    Ey_pred_phys = -dVdy_n / (L_char_y * um_to_m)\n",
    "\n",
    "    X_np, Y_np = X_phys.cpu().detach().numpy(), Y_phys.cpu().detach().numpy()\n",
    "    V_pred_np, Ex_pred_np, Ey_pred_np = V_pred.cpu().detach().numpy(), Ex_pred_phys.cpu().detach().numpy(), Ey_pred_phys.cpu().detach().numpy()\n",
    "    V_comsol_np, Ex_comsol_np, Ey_comsol_np = V_comsol.cpu().numpy(), Ex_comsol.cpu().numpy(), Ey_comsol.cpu().numpy()\n",
    "\n",
    "    L_n = torch.tensor(L / L_char_x); w_n = torch.tensor(w / L_char_x); d_n = torch.tensor(d / L_char_x)\n",
    "    is_in_air_mask = ~is_inside_plates_parametric(torch.from_numpy(X_n_plot.cpu().detach().numpy()), torch.from_numpy(Y_n_plot.cpu().detach().numpy()), L_n, w_n, d_n, L_char_x, L_char_y).numpy()\n",
    "    \n",
    "    \n",
    "    error_V = np.abs(V_pred_np - V_comsol_np)\n",
    "    error_Ex = np.abs(Ex_pred_np - Ex_comsol_np)\n",
    "    error_Ey = np.abs(Ey_pred_np - Ey_comsol_np)\n",
    "    \n",
    "    plate_x_range = [-L/2, L/2]\n",
    "    top_plate_y_range = [d/2, d/2+w]\n",
    "    bottom_plate_y_range = [-d/2-w, -d/2]\n",
    "\n",
    "    plot_tasks = [\n",
    "        {'name': ' V', 'pred': V_pred_np, 'gt': V_comsol_np, 'err': error_V, 'label': 'Potential (V)', 'err_cmap': 'viridis'},\n",
    "        {'name': ' Ex', 'pred': Ex_pred_np, 'gt': Ex_comsol_np, 'err': error_Ex, 'label': 'Ex (V/m)', 'err_cmap': 'magma'},\n",
    "        {'name': ' Ey', 'pred': Ey_pred_np, 'gt': Ey_comsol_np, 'err': error_Ey, 'label': 'Ey (V/m)', 'err_cmap': 'plasma'}\n",
    "    ]\n",
    "\n",
    "    for task in plot_tasks:\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(21, 6), constrained_layout=True)\n",
    "        fig.suptitle(f'{task[\"name\"]} Precision Analysis (L={L}, w={w}, d={d})', fontsize=16)\n",
    "        pred_data_air_only = np.where(is_in_air_mask, task['pred'], np.nan)\n",
    "        gt_data_air_only = task['gt']\n",
    "        error_data = np.abs(pred_data_air_only - gt_data_air_only)\n",
    "        vmin = np.nanmin(gt_data_air_only)\n",
    "        vmax = np.nanmax(gt_data_air_only)\n",
    "        ax = axes[0]; ax.set_title(f'PINN Prediction'); ax.set_ylabel('y (μm)'); ax.set_xlabel('x (μm)')\n",
    "        im = ax.pcolor(X_np, Y_np, pred_data_air_only, cmap='jet', shading='auto', vmin=vmin, vmax=vmax)\n",
    "        cbar = fig.colorbar(im, ax=ax, shrink=0.8); cbar.set_label(task['label']); customize_colorbar_ticks(cbar, pred_data_air_only)\n",
    "        if 'Ex' in task['name']: clim = np.nanmax(np.abs(gt_data_air_only)); cbar.mappable.set_clim(-clim, clim)\n",
    "        ax = axes[1]; ax.set_title(f'COMSOL Ground Truth'); ax.set_xlabel('x (μm)')\n",
    "        im = ax.pcolor(X_np, Y_np, gt_data_air_only, cmap='jet', shading='auto', vmin=vmin, vmax=vmax)\n",
    "        cbar = fig.colorbar(im, ax=ax, shrink=0.8); cbar.set_label(task['label']); customize_colorbar_ticks(cbar, gt_data_air_only)\n",
    "        if 'Ex' in task['name']: clim = np.nanmax(np.abs(gt_data_air_only)); cbar.mappable.set_clim(-clim, clim)\n",
    "        ax = axes[2]; ax.set_title(f'Absolute Error'); ax.set_xlabel('x (μm)')\n",
    "        im = ax.pcolor(X_np, Y_np, error_data, cmap=task['err_cmap'], shading='auto')\n",
    "        cbar = fig.colorbar(im, ax=ax, shrink=0.8); cbar.set_label(f'Error'); customize_colorbar_ticks(cbar, error_data)\n",
    "\n",
    "        for ax_ in axes:\n",
    "             ax_.add_patch(plt.Rectangle((-L/2, d/2), L, w, fill=True, color='gray', zorder=10))\n",
    "             ax_.add_patch(plt.Rectangle((-L/2, -d/2-w), L, w, fill=True, color='gray', zorder=10))\n",
    "        plt.show()\n",
    "\n",
    "    _, C_pinn_U_fF = calculate_capacitance_parametric_final(model, L, w, d, L_char_x, L_char_y)\n",
    "    error_U = 100 * abs(C_pinn_U_fF - C_comsol_fF) / C_comsol_fF\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*20 + f\" Capacitance Analysis (L={L}, w={w}, d={d}) \" + \"-\"*20)\n",
    "    print(f\"  COMSOL Ground Truth: {C_comsol_fF:.4f} fF\")\n",
    "    print(f\"  PINN Predicted Capacitance: {C_pinn_U_fF:.4f} fF (Error: {error_U:.2f} %)\")\n",
    "    print(\"-\" * (60 + len(f\" (L={L}, w={w}, d={d}) \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b9c8c8-5f69-46b3-985f-d2fc162587a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_param_sets = [\n",
    "    {'L': 40, 'w': 2, 'd': 2}, {'L': 40, 'w': 3.3, 'd': 2}, {'L': 40, 'w': 4.7, 'd': 2}, {'L': 40, 'w': 6, 'd': 2},\n",
    "    {'L': 63.3, 'w': 2, 'd': 2}, {'L': 63.3, 'w': 3.3, 'd': 2}, {'L': 63.3, 'w': 4.7, 'd': 2}, {'L': 63.3, 'w': 6, 'd': 2},\n",
    "    {'L': 86.7, 'w': 2, 'd': 2}, {'L': 86.7, 'w': 3.3, 'd': 2}, {'L': 86.7, 'w': 4.7, 'd': 2}, {'L': 86.7, 'w': 6, 'd': 2},\n",
    "    {'L': 110, 'w': 2, 'd': 2}, {'L': 110, 'w': 3.3, 'd': 2}, {'L': 110, 'w': 4.7, 'd': 2}, {'L': 110, 'w': 6, 'd': 2},\n",
    "    {'L': 40, 'w': 2, 'd': 4}, {'L': 40, 'w': 3.3, 'd': 4}, {'L': 40, 'w': 4.7, 'd': 4}, {'L': 40, 'w': 6, 'd': 4},\n",
    "    {'L': 63.3, 'w': 2, 'd': 4}, {'L': 63.3, 'w': 3.3, 'd': 4}, {'L': 63.3, 'w': 4.7, 'd': 4}, {'L': 63.3, 'w': 6, 'd': 4},\n",
    "    {'L': 86.7, 'w': 2, 'd': 4}, {'L': 86.7, 'w': 3.3, 'd': 4}, {'L': 86.7, 'w': 4.7, 'd': 4}, {'L': 86.7, 'w': 6, 'd': 4},\n",
    "    {'L': 110, 'w': 2, 'd': 4}, {'L': 110, 'w': 3.3, 'd': 4}, {'L': 110, 'w': 4.7, 'd': 4}, {'L': 110, 'w': 6, 'd': 4},\n",
    "    {'L': 40, 'w': 2, 'd': 6}, {'L': 40, 'w': 3.3, 'd': 6}, {'L': 40, 'w': 4.7, 'd': 6}, {'L': 40, 'w': 6, 'd': 6},\n",
    "    {'L': 63.3, 'w': 2, 'd': 6}, {'L': 63.3, 'w': 3.3, 'd': 6}, {'L': 63.3, 'w': 4.7, 'd': 6}, {'L': 63.3, 'w': 6, 'd': 6},\n",
    "    {'L': 86.7, 'w': 2, 'd': 6}, {'L': 86.7, 'w': 3.3, 'd': 6}, {'L': 86.7, 'w': 4.7, 'd': 6}, {'L': 86.7, 'w': 6, 'd': 6},\n",
    "    {'L': 110, 'w': 2, 'd': 6}, {'L': 110, 'w': 3.3, 'd': 6}, {'L': 110, 'w': 4.7, 'd': 6}, {'L': 110, 'w': 6, 'd': 6},\n",
    "    {'L': 40, 'w': 2, 'd': 8}, {'L': 40, 'w': 3.3, 'd': 8}, {'L': 40, 'w': 4.7, 'd': 8}, {'L': 40, 'w': 6, 'd': 8},\n",
    "    {'L': 63.3, 'w': 2, 'd': 8}, {'L': 63.3, 'w': 3.3, 'd': 8}, {'L': 63.3, 'w': 4.7, 'd': 8}, {'L': 63.3, 'w': 6, 'd': 8},\n",
    "    {'L': 86.7, 'w': 2, 'd': 8}, {'L': 86.7, 'w': 3.3, 'd': 8}, {'L': 86.7, 'w': 4.7, 'd': 8}, {'L': 86.7, 'w': 6, 'd': 8},\n",
    "    {'L': 110, 'w': 2, 'd': 8}, {'L': 110, 'w': 3.3, 'd': 8}, {'L': 110, 'w': 4.7, 'd': 8}, {'L': 110, 'w': 6, 'd': 8},\n",
    "]\n",
    "\n",
    "val_param_sets = [\n",
    "    {'L': 75, 'w': 6, 'd': 8}, {'L': 90, 'w': 3, 'd': 4},\n",
    "    {'L': 40, 'w': 2, 'd': 18}, {'L': 80, 'w': 10, 'd': 2},\n",
    "    {'L': 80, 'w': 5, 'd': 5},\n",
    "]\n",
    "test_param_sets = [\n",
    "    {'L': 100, 'w': 4, 'd': 2},  {'L': 90, 'w': 3, 'd': 2},\n",
    "    {'L': 110, 'w': 4, 'd': 3},  {'L': 80, 'w': 3, 'd': 2.5},\n",
    "]\n",
    "sets_to_process = {\n",
    "    \"Training Set\": train_param_sets,\n",
    "    \"Validation Set\": val_param_sets,\n",
    "    \"Test Set\": test_param_sets,\n",
    "}\n",
    "\n",
    "model_path = 'final_parametric_model.pth'\n",
    "try:\n",
    "    pinn_model_eval = ParametricPINN(num_layers=NUM_LAYERS, hidden_size=HIDDEN_SIZE).to(device)\n",
    "    pinn_model_eval.load_state_dict(torch.load(model_path, map_location=device, weights_only=True))\n",
    "    pinn_model_eval.eval()\n",
    "    print(\"Model loaded successfully\")\n",
    "\n",
    "    for set_name, param_list in sets_to_process.items():\n",
    "        print(\"\\n\" + \"=\"*80 + f\"\\n     start verification.: {set_name}\\n\" + \"=\"*80)\n",
    "        all_truth_data = preload_comsol_data(param_list, device)\n",
    "        if all_truth_data:\n",
    "            for params in param_list:\n",
    "                if (params['L'], params['w'], params['d']) in all_truth_data:\n",
    "                    analyze_gradient_model_case(pinn_model_eval, L=params['L'], w=params['w'], d=params['d'], ground_truth=all_truth_data, L_char_x=L_char_x, L_char_y=L_char_y)\n",
    "        else:\n",
    "            print(f\"No ground truth data for '{set_name}' \")\n",
    "except NameError as e:\n",
    "    print(f\" NameError: {e}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\" '{model_path}'not found\")\n",
    "except Exception as e:\n",
    "    print(f\"error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
